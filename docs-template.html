<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-Tokenizer Technical Documentation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5.2.0/github-markdown.min.css">
    <style>
        .markdown-body { box-sizing: border-box; min-width: 200px; max-width: 1200px; margin: 0 auto; padding: 45px; }
        @media (max-width: 767px) { .markdown-body { padding: 15px; } }
        .nav-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 40px 0; }
        .nav-card { background: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; text-decoration: none; color: #24292e; transition: all 0.2s ease; }
        .nav-card:hover { border-color: #0366d6; box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-2px); }
        .nav-card h3 { margin-top: 0; color: #0366d6; }
        .header { background: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 30px; margin-bottom: 30px; }
        .status { background: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin: 20px 0; }
        .last-updated { color: #6a737d; font-size: 0.9em; }
        .tech-specs { background: #f1f8ff; border: 1px solid #c8e1ff; border-radius: 6px; padding: 20px; margin: 20px 0; }
        .code-example { background: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 16px; margin: 16px 0; overflow-x: auto; }
    </style>
</head>
<body class="markdown-body">
    <div class="header">
        <h1>ğŸ”§ GPT-Tokenizer Technical Documentation</h1>
        <p>A comprehensive implementation of tokenization algorithms used in Large Language Models, with focus on Byte Pair Encoding (BPE), UTF-8 handling, and pattern matching.</p>
    </div>
    
    <div class="status">
        <strong>ğŸ“… Last Updated:</strong> <span class="last-updated" id="last-updated"></span><br>
        <strong>ğŸ”„ Auto-Deploy:</strong> Documentation automatically updates with every commit<br>
        <strong>ğŸ“Š Version:</strong> Latest from main branch
    </div>
    
    <div class="tech-specs">
        <h3>ğŸš€ Technical Specifications</h3>
        <ul>
            <li><strong>Algorithm:</strong> Byte Pair Encoding (BPE) with GPT-2 style patterns</li>
            <li><strong>Encoding:</strong> UTF-8 byte-level processing</li>
            <li><strong>Performance:</strong> O(t Ã— log v) encoding, O(t) decoding</li>
            <li><strong>Memory:</strong> O(v) vocabulary storage, O(m) merge rules</li>
            <li><strong>Languages:</strong> Python 3.12+, Jupyter notebook support</li>
        </ul>
    </div>
    
    <h2>ğŸ“š Documentation Navigation</h2>
    <div class="nav-grid">
        <a href="README.md" class="nav-card">
            <h3>ğŸ“‹ Technical Overview</h3>
            <p>System architecture, implementation details, and core algorithms</p>
        </a>
        <a href="ARCHITECTURE.md" class="nav-card">
            <h3>ğŸ—ï¸ System Architecture</h3>
            <p>Component design, data flow, memory management, and performance characteristics</p>
        </a>
        <a href="API.md" class="nav-card">
            <h3>ğŸ“– API Reference</h3>
            <p>Complete function documentation, class interfaces, and usage examples</p>
        </a>
        <a href="scripts/tokenisation.ipynb" class="nav-card">
            <h3>ğŸ”§ Interactive Implementation</h3>
            <p>Jupyter notebook with working BPE algorithm and examples</p>
        </a>
        <a href="scripts/" class="nav-card">
            <h3>ğŸ“ Source Code</h3>
            <p>Python implementations, utility functions, and test files</p>
        </a>
        <a href="imgs/tokenizer_diagram_final.svg" class="nav-card">
            <h3>ğŸ“Š System Diagrams</h3>
            <p>Architecture diagrams and data flow visualizations</p>
        </a>
    </div>
    
    <h2>ğŸ” Core Implementation</h2>
    
    <h3>BPE Algorithm</h3>
    <div class="code-example">
        <pre><code>class GPT4BPETokenizer:
    def __init__(self, vocab_size: int = 50000):
        self.vocab_size = vocab_size
        self.vocab = {}  # token_id -> token_string
        self.merges = {}  # (byte1, byte2) -> new_token_id
        
    def train(self, texts: List[str]) -> None:
        # Initialize with individual bytes
        # Find most frequent byte pairs
        # Replace pairs with new tokens
        # Repeat until vocabulary size reached
        
    def encode(self, text: str) -> List[int]:
        # Convert text to UTF-8 bytes
        # Apply learned merge rules
        # Return token IDs</code></pre>
    </div>
    
    <h3>UTF-8 Processing</h3>
    <div class="code-example">
        <pre><code># Text to bytes conversion
text = "Hello world!"
bytes_seq = text.encode('utf-8')
# Result: [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]

# Byte pair analysis
pairs = [(bytes_seq[i], bytes_seq[i+1]) for i in range(len(bytes_seq)-1)]
# Result: [(72,101), (101,108), (108,108), (108,111), ...]</code></pre>
    </div>
    
    <h3>Pattern Matching</h3>
    <div class="code-example">
        <pre><code># GPT-2 style regex pattern
pattern = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

# Pattern components:
# 's|'t|'re|'ve|'m|'ll|'d  - Contractions
# ?\p{L}+                   - Optional space + letters
# ?\p{N}+                   - Optional space + numbers
# ?[^\s\p{L}\p{N}]+        - Optional space + punctuation
# \s+(?!\S)|\s+            - Whitespace sequences</code></pre>
    </div>
    
    <h2>ğŸ“Š Performance Characteristics</h2>
    
    <h3>Time Complexity</h3>
    <ul>
        <li><strong>Training:</strong> O(n Ã— m Ã— vÂ²) where n=corpus size, m=text length, v=vocabulary size</li>
        <li><strong>Encoding:</strong> O(t Ã— (r + log v)) where t=text length, r=merge rules, v=vocabulary size</li>
        <li><strong>Decoding:</strong> O(t) where t=number of tokens</li>
    </ul>
    
    <h3>Memory Usage</h3>
    <ul>
        <li><strong>Vocabulary:</strong> O(v) where v is vocabulary size</li>
        <li><strong>Merge Rules:</strong> O(m) where m is number of merge operations</li>
        <li><strong>Working Memory:</strong> O(t) where t is text length</li>
    </ul>
    
    <h2>ğŸ§ª Testing & Validation</h2>
    
    <h3>Unit Tests</h3>
    <div class="code-example">
        <pre><code>def test_bpe_training():
    tokenizer = GPT4BPETokenizer(vocab_size=100)
    texts = ["Hello world", "Goodbye world"]
    tokenizer.train(texts)
    
    assert len(tokenizer.vocab) <= 100
    assert len(tokenizer.merges) > 0

def test_encode_decode_roundtrip():
    tokenizer = GPT4BPETokenizer()
    original_text = "Hello world!"
    
    tokens = tokenizer.encode(original_text)
    decoded_text = tokenizer.decode(tokens)
    
    assert decoded_text == original_text</code></pre>
    </div>
    
    <h3>Performance Benchmarks</h3>
    <div class="code-example">
        <pre><code>def benchmark_encoding():
    import time
    
    tokenizer = GPT4BPETokenizer()
    test_text = "x" * 10000
    
    start_time = time.time()
    tokens = tokenizer.encode(test_text)
    end_time = time.time()
    
    encoding_time = end_time - start_time
    tokens_per_second = len(test_text) / encoding_time
    
    print(f"Encoding speed: {tokens_per_second:.2f} chars/second")</code></pre>
    </div>
    
    <h2>ğŸš€ Quick Start</h2>
    
    <div class="code-example">
        <pre><code># Clone and setup
git clone https://github.com/DeconvFFT/Gpt-tokenizer.git
cd gpt-tokenizer
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Start with interactive notebook
jupyter notebook scripts/tokenisation.ipynb

# Or use Python directly
from scripts.gpt4_bpe_tokenizer import GPT4BPETokenizer

tokenizer = GPT4BPETokenizer(vocab_size=1000)
tokenizer.train(["Hello world!", "Machine learning is fascinating."])
tokens = tokenizer.encode("Hello AI!")
print(tokens)  # [15496, 995, 0]</code></pre>
    </div>
    
    <h2>ğŸ”§ Development</h2>
    
    <h3>Code Style</h3>
    <ul>
        <li><strong>Formatting:</strong> Black code formatter</li>
        <li><strong>Linting:</strong> Flake8 with max line length 88</li>
        <li><strong>Type Hints:</strong> Required for all public functions</li>
        <li><strong>Documentation:</strong> Google-style docstrings</li>
    </ul>
    
    <h3>Testing</h3>
    <ul>
        <li><strong>Unit Tests:</strong> pytest framework</li>
        <li><strong>Coverage:</strong> Minimum 90% code coverage</li>
        <li><strong>Performance:</strong> Benchmarks for critical paths</li>
        <li><strong>Memory:</strong> Memory usage validation</li>
    </ul>
    
    <h2>ğŸ“ Project Structure</h2>
    
    <div class="code-example">
        <pre><code>gpt-tokenizer/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ tokenisation.ipynb          # Main implementation notebook
â”‚   â”œâ”€â”€ gpt4_bpe_tokenizer.py      # BPE algorithm implementation
â”‚   â”œâ”€â”€ regex_tokenizer.py          # Regex-based tokenization
â”‚   â”œâ”€â”€ basic_tokenizer.py          # Basic tokenization utilities
â”‚   â”œâ”€â”€ helpers.py                  # Utility functions
â”‚   â””â”€â”€ monitor-builds.sh           # Build monitoring script
â”œâ”€â”€ imgs/                           # Architecture diagrams
â”œâ”€â”€ tests/                          # Unit tests and validation
â”œâ”€â”€ docs/                           # Technical documentation
â”œâ”€â”€ .github/workflows/              # CI/CD automation
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                       # Project overview</code></pre>
    </div>
    
    <hr>
    <p><em>Technical documentation for developers and researchers</em></p>
    <p><small>Documentation automatically updated with every commit via GitHub Pages</small></p>
    
    <script>
        // Update last updated timestamp
        document.getElementById('last-updated').textContent = new Date().toLocaleString();
        
        // Add click tracking for analytics (optional)
        document.querySelectorAll('.nav-card').forEach(card => {
            card.addEventListener('click', function() {
                console.log('Navigation clicked:', this.querySelector('h3').textContent);
            });
        });
    </script>
</body>
</html>

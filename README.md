# GPT-Tokenizer

A comprehensive implementation of tokenization algorithms used in Large Language Models, including Byte Pair Encoding (BPE), UTF-8 handling, and interactive examples.

## ğŸš€ Features

- **BPE Implementation**: Complete Byte Pair Encoding algorithm from scratch
- **UTF-8 Support**: Proper handling of Unicode and multi-byte characters
- **Regex Tokenization**: GPT-2 style pattern matching for text segmentation
- **Interactive Learning**: Jupyter notebook with step-by-step examples
- **Production Ready**: Integration with tiktoken for real-world applications

## ğŸ“š Documentation

- **[Diagram Creation Prompt](diagram_creation_prompt.md)** - Optimized prompt for creating professional diagrams
- **[Usage Guide](usage_guide.md)** - How to use the tokenization tools
- **[GitHub Pages Setup](GITHUB_PAGES_SETUP.md)** - Enable automatic documentation deployment

## ğŸ”§ Quick Start

```bash
# Clone and setup
git clone https://github.com/yourusername/gpt-tokenizer.git
cd gpt-tokenizer
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Start learning
jupyter notebook scripts/tokenisation.ipynb
```

## ğŸŒ Live Documentation

This project automatically deploys documentation to GitHub Pages with every commit. To enable:

1. Go to repository Settings â†’ Pages
2. Select "GitHub Actions" as source
3. Documentation will be available at: `https://yourusername.github.io/gpt-tokenizer`

## ğŸ“ Project Structure

```
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ tokenisation.ipynb    # Main Jupyter notebook
â”‚   â”œâ”€â”€ gpt4_bpe_tokenizer.py # BPE tokenizer implementation
â”‚   â”œâ”€â”€ regex_tokenizer.py    # Regex-based tokenizer
â”‚   â”œâ”€â”€ basic_tokenizer.py    # Basic tokenizer
â”‚   â””â”€â”€ helpers.py            # Utility functions
â”œâ”€â”€ imgs/                     # Diagrams and visualizations
â”œâ”€â”€ tests/                    # Test files
â””â”€â”€ .github/workflows/        # GitHub Actions for automation
```

## ğŸ¯ What You'll Learn

- **Tokenization Fundamentals**: How text becomes numbers for AI models
- **BPE Algorithm**: Step-by-step implementation of the compression algorithm
- **Unicode Handling**: Proper text encoding and decoding
- **Vocabulary Building**: Creating and managing token vocabularies
- **Real-world Applications**: Using production tokenizers like tiktoken

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for the tiktoken library
- The AI/ML community for inspiration and feedback

---

**Made with â¤ï¸ for the AI/ML community**
name: Deploy to GitHub Pages

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Build documentation
        run: |
          # Create docs directory for GitHub Pages
          mkdir -p docs
          
          # Copy README as index
          cp README.md docs/index.md
          
          # Copy other documentation files
          cp diagram_creation_prompt.md docs/
          cp usage_guide.md docs/
          
          # Copy images
          mkdir -p docs/imgs
          cp imgs/*.svg docs/imgs/
          
          # Copy scripts
          mkdir -p docs/scripts
          cp scripts/*.py docs/scripts/
          cp scripts/*.ipynb docs/scripts/
          
          # Create a simple HTML index
          cat > docs/index.html << 'EOF'
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>GPT-Tokenizer Documentation</title>
              <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5.2.0/github-markdown.min.css">
              <style>
                  .markdown-body { box-sizing: border-box; min-width: 200px; max-width: 980px; margin: 0 auto; padding: 45px; }
                  @media (max-width: 767px) { .markdown-body { padding: 15px; } }
                  .nav-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 40px 0; }
                  .nav-card { background: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; text-decoration: none; color: #24292e; transition: all 0.2s ease; }
                  .nav-card:hover { border-color: #0366d6; box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-2px); }
                  .nav-card h3 { margin-top: 0; color: #0366d6; }
              </style>
          </head>
          <body class="markdown-body">
              <h1>üöÄ GPT-Tokenizer</h1>
              <p>Your complete guide to understanding and implementing tokenization algorithms used in Large Language Models</p>
              
              <h2>üìö Quick Navigation</h2>
              <div class="nav-grid">
                  <a href="index.md" class="nav-card">
                      <h3>üìã Project Overview</h3>
                      <p>Main project README and documentation</p>
                  </a>
                  <a href="diagram_creation_prompt.md" class="nav-card">
                      <h3>üé® Diagram Creation</h3>
                      <p>Optimized prompt for creating professional diagrams</p>
                  </a>
                  <a href="usage_guide.md" class="nav-card">
                      <h3>üìñ Usage Guide</h3>
                      <p>How to use the tokenization tools</p>
                  </a>
                  <a href="scripts/tokenisation.ipynb" class="nav-card">
                      <h3>üîß Interactive Notebook</h3>
                      <p>Jupyter notebook with examples</p>
                  </a>
              </div>
              
              <h2>üîç What You'll Learn</h2>
              <ul>
                  <li><strong>Tokenization Fundamentals</strong>: How text becomes numbers for AI models</li>
                  <li><strong>BPE Algorithm</strong>: Step-by-step implementation of the compression algorithm</li>
                  <li><strong>Unicode Handling</strong>: Proper text encoding and decoding</li>
                  <li><strong>Vocabulary Building</strong>: Creating and managing token vocabularies</li>
                  <li><strong>Real-world Applications</strong>: Using production tokenizers like tiktoken</li>
              </ul>
              
              <h2>üöÄ Quick Start</h2>
              <pre><code># Clone and setup
git clone https://github.com/yourusername/gpt-tokenizer.git
cd gpt-tokenizer
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Start learning
jupyter notebook scripts/tokenisation.ipynb</code></pre>
              
              <hr>
              <p><em>Made with ‚ù§Ô∏è for the AI/ML community</em></p>
              <p><small>Documentation automatically updated with every commit via GitHub Pages</small></p>
          </body>
          </html>
          EOF
          
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
